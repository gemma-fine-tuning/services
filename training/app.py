import os
import logging
from typing import Optional, List
from fastapi import FastAPI, HTTPException, Depends
from google.cloud import run_v2
from google.cloud import storage
from datetime import timedelta, datetime, timezone
from auth import initialize_firebase, get_current_user_id
from schema import (
    TrainRequest,
    JobSubmitResponse,
    JobStatusResponse,
    JobListResponse,
    JobListEntry,
    DownloadUrlResponse,
    JobDeleteResponse,
)
import json
import uvicorn
import hashlib
from job_manager import JobStateManager, JobMetadata, JobStatus

app = FastAPI(
    title="Gemma Training Service",
    version="1.0.0",
    description="Training backend (Cloud Run GPU) for fine-tuning LLMs with various methods",
)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logging.info("âœ… Training service ready")

# Initialize Firebase
initialize_firebase()

project_id = os.getenv("PROJECT_ID")
if not project_id:
    raise ValueError("PROJECT_ID environment variable must be set for Firestore client")
job_manager = JobStateManager(project_id)

# These will not be configured in the os envvars because they are pretty much fixed to these two values
REGION = os.getenv("REGION", "us-central1")
JOB_NAME = os.getenv("TRAINING_JOB_NAME", "training-job")
GCS_CONFIG_BUCKET = os.getenv("GCS_CONFIG_BUCKET_NAME", "gemma-train-config")
GCS_EXPORT_BUCKET = os.getenv("GCS_EXPORT_BUCKET_NAME", "gemma-export-bucket")


@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "training"}


def make_job_id(
    processed_dataset_id: str, base_model_id: str, request: TrainRequest
) -> str:
    """
    Creates a job ID for a training job.

    This has the following properties:
    - IDs are only the same if the processed dataset ID, base model ID, and request are the same
    - IDs are deterministic, i.e. same request will always produce the same ID
    - IDs are short because we truncate the hash to 8 characters

    This is desired because the exact same job will overwrite the previous one and avoid confusion and resource waste.
    This also makes tracking in firestore consistent.

    Args:
        processed_dataset_id (str): processed_dataset_id, this might contain spaces we need to replace them first
        base_model_id (str): The ID of the base model
        request (TrainRequest): The request object

    Returns:
        str: The job ID
    """
    base = f"training_{processed_dataset_id.replace(' ', '-')}_{base_model_id.split('/')[-1]}"
    request_str = json.dumps(request.model_dump(), sort_keys=True)
    short_hash = hashlib.sha256(request_str.encode()).hexdigest()[:8]
    return f"{base}_{short_hash}"


@app.get("/jobs", response_model=JobListResponse)
async def list_jobs(current_user_id: str = Depends(get_current_user_id)):
    """List all jobs with job_id and job_name."""
    try:
        # Query Firestore for jobs owned by user
        docs = job_manager.collection.where("user_id", "==", current_user_id).stream()
        entries = []
        for doc in docs:
            data = doc.to_dict() or {}
            entries.append(
                JobListEntry(
                    job_id=data.get("job_id"),
                    job_name=data.get("job_name"),
                    base_model_id=data.get("base_model_id"),
                    status=data.get("status", "unknown"),
                    modality=data.get("modality", "text"),
                )
            )
        return JobListResponse(jobs=entries)
    except Exception as e:
        logging.error(f"Failed to list jobs: {e}")
        raise HTTPException(status_code=500, detail="Failed to list jobs")


@app.post("/train", response_model=JobSubmitResponse)
async def start_training(
    request: TrainRequest,
    current_user_id: str = Depends(get_current_user_id),
):
    processed_dataset_id = request.processed_dataset_id

    # Verify ownership of the processed dataset before starting the job
    if not job_manager.verify_processed_dataset_ownership(
        processed_dataset_id, current_user_id
    ):
        raise HTTPException(status_code=404, detail="Processed dataset not found")

    base_model_id = request.training_config.base_model_id
    job_id = make_job_id(processed_dataset_id, base_model_id, request)

    if (
        request.export_config.destination == "hfhub"
        and not request.export_config.hf_repo_id
    ):
        raise HTTPException(
            status_code=400,
            detail="hf_repo_id is required when export is hfhub",
        )

    # Immediately create job record in Firestore
    job_metadata = JobMetadata(
        job_id=job_id,
        status=JobStatus.QUEUED,
        created_at=datetime.now(timezone.utc),
        updated_at=datetime.now(timezone.utc),
        processed_dataset_id=request.processed_dataset_id,
        base_model_id=request.training_config.base_model_id,
        job_name=request.job_name,
        modality=request.training_config.modality,
        user_id=current_user_id,
    )
    job_manager.ensure_job_document_exists(job_id, job_metadata)

    # Upload config to GCS
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_CONFIG_BUCKET)
        blob = bucket.blob(f"{job_id}.json")
        blob.upload_from_string(
            json.dumps(request.model_dump()), content_type="application/json"
        )
        logging.info(
            f"Uploaded training config to gs://{GCS_CONFIG_BUCKET}/{job_id}.json"
        )
    except Exception as e:
        logging.error(f"Failed to upload config to GCS: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Failed to upload config to GCS: {str(e)}"
        )

    try:
        client = run_v2.JobsClient()
        job_name = f"projects/{project_id}/locations/{REGION}/jobs/{JOB_NAME}"
        run_request = run_v2.RunJobRequest(
            name=job_name,
            overrides=run_v2.RunJobRequest.Overrides(
                container_overrides=[
                    run_v2.RunJobRequest.Overrides.ContainerOverride(
                        env=[run_v2.EnvVar(name="JOB_ID", value=job_id)]
                    )
                ]
            ),
        )
        _ = client.run_job(request=run_request)

        # NOTE: We don't need to wait for the job to complete
        # because the job will be tracked in firestore
        # If needed you can do response = operation.result()

        return JobSubmitResponse(job_id=job_id)
    except Exception as e:
        logging.error(f"Failed to start training job: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Failed to start training job: {str(e)}"
        )


@app.get("/jobs/{job_id}/download/gguf", response_model=DownloadUrlResponse)
async def download_gguf_file(
    job_id: str,
    current_user_id: str = Depends(get_current_user_id),
):
    """
    Generate a signed URL for downloading the GGUF file of a specific job.
    This is a convenience endpoint that automatically finds the GGUF file path from job status.
    """
    try:
        # Verify ownership
        if not job_manager.verify_job_ownership(job_id, current_user_id):
            raise HTTPException(status_code=404, detail="Job not found")
        # Get job status to find GGUF path
        job_data = job_manager.get_job_status_dict(job_id)
        if not job_data:
            raise HTTPException(status_code=404, detail="Job not found")

        gguf_path = job_data.get("gguf_path")
        if not gguf_path:
            raise HTTPException(
                status_code=404,
                detail="No GGUF file available for this job. Check if include_gguf was enabled during training.",
            )

        # Extract the blob path from the GCS URL
        # gguf_path format: gs://bucket/gguf_models/job_123/model.gguf
        if not gguf_path.startswith("gs://"):
            raise HTTPException(status_code=500, detail="Invalid GGUF path format")

        # Remove gs://bucket/ prefix to get blob path
        path_parts = gguf_path.replace("gs://", "").split("/", 1)
        if len(path_parts) != 2:
            raise HTTPException(status_code=500, detail="Invalid GGUF path format")

        bucket_name, blob_path = path_parts

        # Verify it's the expected bucket
        if bucket_name != GCS_EXPORT_BUCKET:
            raise HTTPException(
                status_code=500, detail="GGUF file in unexpected bucket"
            )

        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_EXPORT_BUCKET)
        blob = bucket.blob(blob_path)

        if not blob.exists():
            raise HTTPException(
                status_code=404, detail="GGUF file not found in storage"
            )

        # Generate a signed URL that is valid for 1 hour
        signed_url = blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=30),
            method="GET",
        )

        return DownloadUrlResponse(download_url=signed_url)

    except HTTPException:
        raise
    except Exception as e:
        logging.error(
            f"Failed to generate GGUF download URL for job {job_id}: {str(e)}"
        )
        raise HTTPException(status_code=500, detail=str(e))


# NOTE: In the future we might want to add /jobs/{job_id}/download/model endpoint for adapter and merged models


def delete_gcs_resources(
    job_id: str, adapter_path: Optional[str], gguf_path: Optional[str]
) -> List[str]:
    """
    Delete GCS resources associated with a job.

    Args:
        job_id: Job identifier
        adapter_path: GCS path to adapter/merged model
        gguf_path: GCS path to GGUF model

    Returns:
        List of deleted resource paths
    """
    deleted_resources = []
    storage_client = storage.Client()

    # Delete config file
    try:
        config_bucket = storage_client.bucket(GCS_CONFIG_BUCKET)
        config_blob = config_bucket.blob(f"{job_id}.json")
        if config_blob.exists():
            config_blob.delete()
            deleted_resources.append(f"gs://{GCS_CONFIG_BUCKET}/{job_id}.json")
            logging.info(f"Deleted config file: gs://{GCS_CONFIG_BUCKET}/{job_id}.json")
    except Exception as e:
        logging.warning(f"Failed to delete config file for job {job_id}: {e}")

    # Delete adapter/merged model
    if adapter_path:
        try:
            if adapter_path.startswith("gs://"):
                # Parse GCS path
                path_parts = adapter_path.replace("gs://", "").split("/", 1)
                if len(path_parts) == 2:
                    bucket_name, blob_prefix = path_parts
                    bucket = storage_client.bucket(bucket_name)

                    # Delete all blobs with this prefix (handles directories)
                    blobs = bucket.list_blobs(prefix=blob_prefix)
                    deleted_count = 0
                    for blob in blobs:
                        blob.delete()
                        deleted_count += 1

                    if deleted_count > 0:
                        deleted_resources.append(
                            f"gs://{bucket_name}/{blob_prefix} ({deleted_count} files)"
                        )
                        logging.info(
                            f"Deleted adapter resources: gs://{bucket_name}/{blob_prefix} ({deleted_count} files)"
                        )
        except Exception as e:
            logging.warning(f"Failed to delete adapter resources for job {job_id}: {e}")

    # Delete GGUF model
    if gguf_path:
        try:
            if gguf_path.startswith("gs://"):
                # Parse GCS path
                path_parts = gguf_path.replace("gs://", "").split("/", 1)
                if len(path_parts) == 2:
                    bucket_name, blob_path = path_parts
                    bucket = storage_client.bucket(bucket_name)
                    blob = bucket.blob(blob_path)

                    if blob.exists():
                        blob.delete()
                        deleted_resources.append(gguf_path)
                        logging.info(f"Deleted GGUF file: {gguf_path}")
        except Exception as e:
            logging.warning(f"Failed to delete GGUF file for job {job_id}: {e}")

    return deleted_resources


@app.delete("/jobs/{job_id}/delete", response_model=JobDeleteResponse)
async def delete_job(
    job_id: str,
    current_user_id: str = Depends(get_current_user_id),
):
    """
    Delete a job and all associated GCS resources.
    This includes the job metadata, config file, adapter/merged model, and GGUF file if available.
    """
    try:
        # Verify ownership
        if not job_manager.verify_job_ownership(job_id, current_user_id):
            raise HTTPException(status_code=404, detail="Job not found")

        # First, get job data to find associated resources
        job_data = job_manager.get_job_status_dict(job_id)
        if not job_data:
            raise HTTPException(status_code=404, detail="Job not found")

        adapter_path = job_data.get("adapter_path")
        gguf_path = job_data.get("gguf_path")

        # Delete GCS resources
        deleted_resources = delete_gcs_resources(job_id, adapter_path, gguf_path)

        # Delete job metadata from Firestore
        job_deleted = job_manager.delete_job(job_id)

        if job_deleted:
            message = f"Successfully deleted job {job_id} and associated resources"
            logging.info(message)
        else:
            message = f"Job {job_id} metadata was not found, but cleaned up any existing resources"
            logging.warning(message)

        return JobDeleteResponse(
            job_id=job_id,
            deleted=True,
            message=message,
            deleted_resources=deleted_resources,
        )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Failed to delete job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to delete job: {str(e)}")


@app.get("/jobs/{job_id}/status", response_model=JobStatusResponse)
async def get_job_status(
    job_id: str,
    current_user_id: str = Depends(get_current_user_id),
):
    # Verify ownership
    if not job_manager.verify_job_ownership(job_id, current_user_id):
        raise HTTPException(status_code=404, detail="Job not found")

    job_data = job_manager.get_job_status_dict(job_id)
    if not job_data:
        logging.error(f"Job not found: {job_id}")
        raise HTTPException(status_code=404, detail="Job not found")

    return job_data


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)
